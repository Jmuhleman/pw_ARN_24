{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e3ea85c-79e6-4ffa-acfb-5d72d34380e0",
   "metadata": {},
   "source": [
    "# Second experiment: awake / n-rem / rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f94744a6-fabd-47e1-b1e0-f341cd3046e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import layers\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "import random as random\n",
    "\n",
    "trainset_1 = pd.read_csv('EEG_mouse_data_1.csv')\n",
    "trainset_2 = pd.read_csv('EEG_mouse_data_2.csv')\n",
    "testset_1 = pd.read_csv('EEG_mouse_data_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bcdd2a9-eb88-44fc-b1ba-a9a83d33cbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normlisation des features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "def normalize(df):\n",
    "    headers = df.columns\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    numerical_columns = df.select_dtypes(include=['int', 'float']).columns\n",
    "    df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "    normalized_data = pd.DataFrame(df, columns=headers[:-1])\n",
    "\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a428b63-1572-4cf4-8c0f-5ae5fdf7b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction des colonnes\n",
    "# envisager de faire la séléction des features ici\n",
    "def extract_features(data, n):\n",
    "    return data.iloc[:,:n+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a417995d-c029-42c1-8790-fd91810b71c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">78</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │              \u001b[38;5;34m78\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │              \u001b[38;5;34m12\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">90</span> (360.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m90\u001b[0m (360.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">90</span> (360.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m90\u001b[0m (360.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import layers\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def create_model_three_class():\n",
    "  # 2 inputs\n",
    "  # 2 hidden nodes\n",
    "  # 1 output\n",
    "\n",
    "  mlp = keras.Sequential([\n",
    "      layers.Input(shape=(25,)),\n",
    "      layers.Dense(3, activation=\"tanh\"), # Try different numbers of hidden neurons here (e.g. 2, 4, 8, 32, 128)\n",
    "      layers.Dense(3, activation=\"tanh\"),\n",
    "  ])\n",
    "\n",
    "  # Experiment with hyperparameters here:\n",
    "  # momentum: [0, 0.8, 0.9, 0.99]\n",
    "  # learning_rate: [0.1, 0.01, 0.001, 0.0001]\n",
    "  mlp.compile(\n",
    "      optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.99),\n",
    "      loss=\"mse\",\n",
    "  )\n",
    "\n",
    "  return mlp\n",
    "mlp = create_model_three_class()\n",
    "mlp.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a267d66e-8134-49bf-a52b-df556619e7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9868 - val_loss: 0.7082\n",
      "Epoch 2/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6878 - val_loss: 0.7056\n",
      "Epoch 3/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6823 - val_loss: 0.7045\n",
      "Epoch 4/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6808 - val_loss: 0.7044\n",
      "Epoch 5/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6793 - val_loss: 0.7036\n",
      "Epoch 6/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6784 - val_loss: 0.7037\n",
      "Epoch 7/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6775 - val_loss: 0.7042\n",
      "Epoch 8/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6770 - val_loss: 0.7052\n",
      "Epoch 9/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6763 - val_loss: 0.7059\n",
      "Epoch 10/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6757 - val_loss: 0.7063\n",
      "Epoch 11/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6757 - val_loss: 0.7079\n",
      "Epoch 12/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6756 - val_loss: 0.7075\n",
      "Epoch 13/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6754 - val_loss: 0.7072\n",
      "Epoch 14/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6749 - val_loss: 0.7071\n",
      "Epoch 15/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6745 - val_loss: 0.7078\n",
      "Epoch 16/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6745 - val_loss: 0.7083\n",
      "Epoch 17/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6744 - val_loss: 0.7086\n",
      "Epoch 18/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6744 - val_loss: 0.7092\n",
      "Epoch 19/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6744 - val_loss: 0.7096\n",
      "Epoch 20/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6741 - val_loss: 0.7100\n",
      "Epoch 21/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6741 - val_loss: 0.7102\n",
      "Epoch 22/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6740 - val_loss: 0.7104\n",
      "Epoch 23/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6739 - val_loss: 0.7104\n",
      "Epoch 24/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6738 - val_loss: 0.7102\n",
      "Epoch 25/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6737 - val_loss: 0.7100\n",
      "Epoch 26/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6735 - val_loss: 0.7097\n",
      "Epoch 27/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6732 - val_loss: 0.7092\n",
      "Epoch 28/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6725 - val_loss: 0.7089\n",
      "Epoch 29/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6722 - val_loss: 0.7091\n",
      "Epoch 30/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6721 - val_loss: 0.7089\n",
      "Epoch 31/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6717 - val_loss: 0.7090\n",
      "Epoch 32/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6718 - val_loss: 0.7092\n",
      "Epoch 33/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6715 - val_loss: 0.7090\n",
      "Epoch 34/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6712 - val_loss: 0.7087\n",
      "Epoch 35/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6708 - val_loss: 0.7089\n",
      "Epoch 36/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6711 - val_loss: 0.7088\n",
      "Epoch 37/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6708 - val_loss: 0.7090\n",
      "Epoch 38/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6707 - val_loss: 0.7086\n",
      "Epoch 39/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6704 - val_loss: 0.7091\n",
      "Epoch 40/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6706 - val_loss: 0.7085\n",
      "Epoch 41/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6703 - val_loss: 0.7090\n",
      "Epoch 42/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6705 - val_loss: 0.7084\n",
      "Epoch 43/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6702 - val_loss: 0.7091\n",
      "Epoch 44/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6705 - val_loss: 0.7082\n",
      "Epoch 45/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6702 - val_loss: 0.7087\n",
      "Epoch 46/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6707 - val_loss: 0.7089\n",
      "Epoch 47/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6701 - val_loss: 0.7085\n",
      "Epoch 48/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6709 - val_loss: 0.7086\n",
      "Epoch 49/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6707 - val_loss: 0.7083\n",
      "Epoch 50/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6699 - val_loss: 0.7079\n",
      "Epoch 1/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9718 - val_loss: 0.6978\n",
      "Epoch 2/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.7015 - val_loss: 0.6905\n",
      "Epoch 3/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6998 - val_loss: 0.6906\n",
      "Epoch 4/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6974 - val_loss: 0.6899\n",
      "Epoch 5/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6968 - val_loss: 0.6892\n",
      "Epoch 6/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6953 - val_loss: 0.6886\n",
      "Epoch 7/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6953 - val_loss: 0.6884\n",
      "Epoch 8/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6948 - val_loss: 0.6890\n",
      "Epoch 9/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6943 - val_loss: 0.6892\n",
      "Epoch 10/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6936 - val_loss: 0.6891\n",
      "Epoch 11/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6929 - val_loss: 0.6894\n",
      "Epoch 12/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6929 - val_loss: 0.6895\n",
      "Epoch 13/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6924 - val_loss: 0.6897\n",
      "Epoch 14/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6917 - val_loss: 0.6893\n",
      "Epoch 15/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6912 - val_loss: 0.6894\n",
      "Epoch 16/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6909 - val_loss: 0.6894\n",
      "Epoch 17/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6909 - val_loss: 0.6896\n",
      "Epoch 18/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6906 - val_loss: 0.6894\n",
      "Epoch 19/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6904 - val_loss: 0.6896\n",
      "Epoch 20/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6904 - val_loss: 0.6894\n",
      "Epoch 21/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6903 - val_loss: 0.6895\n",
      "Epoch 22/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6905 - val_loss: 0.6894\n",
      "Epoch 23/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6904 - val_loss: 0.6894\n",
      "Epoch 24/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6903 - val_loss: 0.6894\n",
      "Epoch 25/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6904 - val_loss: 0.6893\n",
      "Epoch 26/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6901 - val_loss: 0.6893\n",
      "Epoch 27/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6900 - val_loss: 0.6897\n",
      "Epoch 28/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6895 - val_loss: 0.6897\n",
      "Epoch 29/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6895 - val_loss: 0.6898\n",
      "Epoch 30/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6895 - val_loss: 0.6898\n",
      "Epoch 31/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6895 - val_loss: 0.6898\n",
      "Epoch 32/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6894 - val_loss: 0.6898\n",
      "Epoch 33/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6892 - val_loss: 0.6899\n",
      "Epoch 34/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6892 - val_loss: 0.6899\n",
      "Epoch 35/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6891 - val_loss: 0.6896\n",
      "Epoch 36/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6890 - val_loss: 0.6896\n",
      "Epoch 37/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6888 - val_loss: 0.6897\n",
      "Epoch 38/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6888 - val_loss: 0.6897\n",
      "Epoch 39/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6886 - val_loss: 0.6900\n",
      "Epoch 40/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6886 - val_loss: 0.6899\n",
      "Epoch 41/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6882 - val_loss: 0.6902\n",
      "Epoch 42/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6882 - val_loss: 0.6900\n",
      "Epoch 43/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6881 - val_loss: 0.6903\n",
      "Epoch 44/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6880 - val_loss: 0.6901\n",
      "Epoch 45/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6881 - val_loss: 0.6906\n",
      "Epoch 46/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6879 - val_loss: 0.6905\n",
      "Epoch 47/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6879 - val_loss: 0.6904\n",
      "Epoch 48/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6880 - val_loss: 0.6910\n",
      "Epoch 49/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6879 - val_loss: 0.6907\n",
      "Epoch 50/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6878 - val_loss: 0.6911\n",
      "Epoch 1/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9334 - val_loss: 0.7046\n",
      "Epoch 2/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.7056 - val_loss: 0.7010\n",
      "Epoch 3/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6983 - val_loss: 0.6979\n",
      "Epoch 4/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6957 - val_loss: 0.6963\n",
      "Epoch 5/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6943 - val_loss: 0.6971\n",
      "Epoch 6/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6946 - val_loss: 0.6961\n",
      "Epoch 7/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6931 - val_loss: 0.6963\n",
      "Epoch 8/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6928 - val_loss: 0.6960\n",
      "Epoch 9/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6922 - val_loss: 0.6957\n",
      "Epoch 10/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6911 - val_loss: 0.6955\n",
      "Epoch 11/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6910 - val_loss: 0.6949\n",
      "Epoch 12/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6904 - val_loss: 0.6950\n",
      "Epoch 13/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6902 - val_loss: 0.6949\n",
      "Epoch 14/50\n",
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6897 - val_loss: 0.6946\n",
      "Epoch 15/50\n",
      "\u001b[1m  1/423\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - loss: 0.5360"
     ]
    }
   ],
   "source": [
    "# 3 classes\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import layers\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "import random as random\n",
    "\n",
    "# Programme principal de traitement du train set #1\n",
    "normal_trainset_1_three_class = extract_features(trainset_1, 26)\n",
    "normal_trainset_1_three_class = normalize(normal_trainset_1_three_class)\n",
    "normal_trainset_1_three_class = normal_trainset_1_three_class.sample(frac=1, random_state=42)\n",
    "\n",
    "# splitting en 3 folds\n",
    "keras.utils.set_random_seed(123)\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "y = normal_trainset_1_three_class['state'].map({'w': 2, 'r': 1, 'n': 0})\n",
    "normal_trainset_1_three_class.drop(columns=['state'], inplace=True)\n",
    "\n",
    "history_list_three_class = []\n",
    "trained_mlp_three_class = []\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(normal_trainset_1_three_class)):\n",
    "    \n",
    "    model = create_model_three_class()\n",
    "    \n",
    "    history = model.fit(x=normal_trainset_1_three_class.iloc[train_index], y=y.iloc[train_index], \n",
    "                       validation_data=(normal_trainset_1_three_class.iloc[test_index], y.iloc[test_index]),\n",
    "                       epochs=50)\n",
    "    \n",
    "    history_list_three_class.append(history)\n",
    "    trained_mlp_three_class.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cf5f7a-cd5b-4b55-8086-2a2ac915cd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "train_losses = np.array([history.history['loss'] for history in history_list_three_class])\n",
    "val_losses = np.array([history.history['val_loss'] for history in history_list_three_class])\n",
    "\n",
    "# Calculate mean and standard deviation for training and validation losses\n",
    "mean_train_loss = np.mean(train_losses, axis=0)\n",
    "std_train_loss = np.std(train_losses, axis=0)\n",
    "mean_val_loss = np.mean(val_losses, axis=0)\n",
    "std_val_loss = np.std(val_losses, axis=0)\n",
    "\n",
    "# Plot mean and standard deviation for training loss\n",
    "pl.plot(mean_train_loss, label='Training Loss (Mean)')\n",
    "pl.fill_between(range(len(mean_train_loss)), mean_train_loss - std_train_loss, mean_train_loss + std_train_loss, alpha=0.3, label='Training Loss (Std)')\n",
    "\n",
    "# Plot mean and standard deviation for validation loss\n",
    "pl.plot(mean_val_loss, label='Validation Loss (Mean)')\n",
    "pl.fill_between(range(len(mean_val_loss)), mean_val_loss - std_val_loss, mean_val_loss + std_val_loss, alpha=0.3, label='Validation Loss (Std)')\n",
    "\n",
    "# Add labels and legend\n",
    "pl.xlabel('Epochs')\n",
    "pl.ylabel('Loss')\n",
    "pl.legend()\n",
    "\n",
    "# Display the plot\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d06271a-a84a-4e07-a6d5-2c696cc82072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 classes\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(confusion_matrix, title):\n",
    "    # Plot confusion matrix\n",
    "    pl.figure(figsize=(8, 6))\n",
    "    sns.heatmap(confusion_matrix.astype(int), annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=[\"n\", \"r\", \"w\"], yticklabels=[\"n\", \"r\", \"w\"])\n",
    "    pl.title(title)\n",
    "    pl.xlabel('Predicted')\n",
    "    pl.ylabel('True')\n",
    "    pl.show()\n",
    "\n",
    "f1_scores = []\n",
    "mean_confusion_matrix = np.zeros((3, 3))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(normal_trainset_1_three_class)):\n",
    "    # Evaluate the trained model on the test fold\n",
    "    predicted_probabilities = trained_mlp_three_class[i].predict(normal_trainset_1_three_class.iloc[test_index])\n",
    "    predictions = np.argmax(predicted_probabilities, axis=1)\n",
    "    true_labels = y.iloc[test_index]\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    mean_confusion_matrix += confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Compute confusion matrix and plot\n",
    "    #plot_confusion_matrix(cm, f'Confusion Matrix - Fold {i + 1}')\n",
    "\n",
    "    # Compute F1 score\n",
    "    f1 = f1_score(true_labels, predictions, average='micro')\n",
    "    f1_scores.append(f1)\n",
    "    print(f\"F1 Score - Fold {i + 1}: {f1}\")\n",
    "\n",
    "# Plot mean confusion matrix\n",
    "plot_confusion_matrix(mean_confusion_matrix, 'Global confusion matrix')\n",
    "\n",
    "# Calculate and display the mean F1 score across all folds\n",
    "mean_f1_score = np.mean(f1_scores)\n",
    "print(f\"Mean F1 Score across all folds: {mean_f1_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
